{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ML4SCI/DeepLearnHackathon/blob/main/ExoplanetSearchChallenge/Exoplanet_Search_Challenge.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Exoplanet Search Challenge"
      ],
      "metadata": {
        "id": "WtB_NjZLo2Sb"
      },
      "id": "WtB_NjZLo2Sb"
    },
    {
      "cell_type": "markdown",
      "source": [
        "<div style=\"text-align: center;\">\n",
        "  <img src=\"https://upload.wikimedia.org/wikipedia/commons/9/9d/HL_Tau_protoplanetary_disk.jpg\" alt=\"HL Tau\" width=\"500\" height=\"500\">\n",
        "</div>\n",
        "\n",
        "Credit: ALMA (ESO/NAOJ/NRAO)"
      ],
      "metadata": {
        "id": "8jex4-KLtf4B"
      },
      "id": "8jex4-KLtf4B"
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## Detecting Exoplanets in Protoplanetary Disks\n",
        "\n",
        "### Description\n",
        "Protoplanetary disks are the sites of planet formation. They provide laboratories against which theories of planet formation can be tested. State-of-the-art telescopes have the power to observe these systems in unprecedented detail. These observations can contain a wealth of information that can be used to advance theories. However, extracting this information can be difficult since the observations are noisy, and there are few well-understood disks. Recently, the interplay of advanced simulations and machine learning have been successful in analyzing these disks and identifying exoplanets [[1](https://ui.adsabs.harvard.edu/abs/2021ApJ...920....3A/abstract)] [[2](https://ui.adsabs.harvard.edu/abs/2022MNRAS.510.4473Z/abstract)] [[3](https://ui.adsabs.harvard.edu/abs/2022ApJ...941..192T/abstract)] [[4](https://ui.adsabs.harvard.edu/abs/2023ApJ...947...60T/abstract)]. This promising avenue of research is the basis for this Hackathon challenge.\n",
        "\n",
        "### Task\n",
        "The task is to train a model that is capable of identifying if a synthetic observation contains a planet. This is a binary classification problem: planet or no planet.\n",
        "\n",
        "### Datasets\n",
        "The data used was generated for [Terry et al. (2022)](https://ui.adsabs.harvard.edu/abs/2022ApJ...941..192T/abstract). It consists of .fits files that represent synthetic continuum observations of protoplanetary disks at 1250 microns. Each simulation, for which there may be several snapshots, consists of a disk with between 0-4 planets. Data includes [the full training dataset](https://drive.google.com/drive/folders/1BV8FksW_EZnLTWUeHwJ_fEctFgjVbhMp?usp=drive_link), [a subset of the training data](https://drive.google.com/file/d/1I0JS1Qd896BGgsPcga3umQm-RuJB37UA/view?usp=drive_link), and [the training labels](https://drive.google.com/file/d/1gtBi4ILvCe8nTF09p_E9WWMplTQGC2Wr/view?usp=drive_link). The labels correspond to the simulation number, e.g., planet0_xxxx.fits corresponds to run 0. Each .fits file comes with 4 channels, but only the first one is relevant. This example only uses the small training subset since this is meant for speed and clarity rather than performance. Final training should be done on the entire training set.\n",
        "\n",
        "### Evaluation Metrics\n",
        "* AUC for withheld test set that will not be given\n",
        "* Performance on real observations\n",
        "\n",
        "## Deliverables\n",
        "* You are required to submit a Google Colab Jupyter Notebook clearly showing your implementation along with the above-mentioned evaluation metrics for test data. This test data is part of the provided data, but it should not be used as training or validation data. It is not the same data that we will test on.\n",
        "* A PDF of your final Jupyter notebook\n",
        "* You must also submit the final trained model, including the model architecture and the trained weights ( For example: HDF5 file, .pb file, .pt file, etc. ) that can be easily implemented on our withheld data.\n"
      ],
      "metadata": {
        "id": "4sfJxUYurgdi"
      },
      "id": "4sfJxUYurgdi"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Imports"
      ],
      "metadata": {
        "id": "ZvsYdJX0oxL1"
      },
      "id": "ZvsYdJX0oxL1"
    },
    {
      "cell_type": "code",
      "source": [
        "# used for downloading data\n",
        "!pip install gdown"
      ],
      "metadata": {
        "id": "cJPk4N1IEH5z"
      },
      "id": "cJPk4N1IEH5z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# used for model\n",
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "id": "1lbzBDaRDBRE"
      },
      "id": "1lbzBDaRDBRE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e18dccc1-aa52-4b4c-8d73-be197443ec4b",
      "metadata": {
        "id": "e18dccc1-aa52-4b4c-8d73-be197443ec4b"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from astropy.io import fits\n",
        "\n",
        "import gdown\n",
        "\n",
        "from matplotlib.colors import LogNorm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import pytorch_lightning as pl\n",
        "from pytorch_lightning.callbacks import LearningRateMonitor, progress\n",
        "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
        "\n",
        "from sklearn.metrics import roc_curve, roc_auc_score\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import torch\n",
        "from torch import nn, Tensor\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import MultiStepLR\n",
        "from torch.utils.data import DataLoader, Dataset, RandomSampler, random_split\n",
        "\n",
        "import torchmetrics\n",
        "\n",
        "import torchvision\n",
        "import torchvision.transforms as T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32c52d89-34df-4f7f-9248-5a378370112c",
      "metadata": {
        "id": "32c52d89-34df-4f7f-9248-5a378370112c"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(123)\n",
        "np.random.seed(123)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e6aeec3-488b-459e-9303-ecb7f11bf8c2",
      "metadata": {
        "id": "5e6aeec3-488b-459e-9303-ecb7f11bf8c2"
      },
      "source": [
        "# Load data"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The below method downloads the data from Google Drive. This will be very slow for when using the entire dataset, so it is recommended that the data is added to your personal Google Drive and mount it using code similar to that below (or do it locally)\n",
        "\n",
        "```python\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "data_dir = \"/content/drive/My Drive/Full_Train_Data/\"\n",
        "data_names = os.listdir(data_dir)\n",
        "label_name = \"/content/drive/My Drive/train_info.csv\"\n",
        "label_df = pd.read_csv(label_name, usecols=range(1, 11))\n",
        "```"
      ],
      "metadata": {
        "id": "69O943J2rK6w"
      },
      "id": "69O943J2rK6w"
    },
    {
      "cell_type": "code",
      "source": [
        "### Sample data subset (NOT full dataset)\n",
        "### The dataset used in this example is a very small subset for the sake of speed\n",
        "### Using only this data would severely overtrain the models\n",
        "### For deployment, the entire training data folder should be used\n",
        "# data_id = \"15AMGfgEu2ltGZN3rVtMV97mSbF2USrs6\" ## for full set\n",
        "data_id = \"1I0JS1Qd896BGgsPcga3umQm-RuJB37UA\"\n",
        "gdown.download(f\"https://drive.google.com/uc?id={data_id}\", \"data_names.zip\", quiet=False)"
      ],
      "metadata": {
        "id": "QLNxyedSEceE"
      },
      "id": "QLNxyedSEceE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# unzip the downloaded data folder\n",
        "!unzip -q data_names.zip -d data_names"
      ],
      "metadata": {
        "id": "45kSCrmsEo3D"
      },
      "id": "45kSCrmsEo3D",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c384828f-0fe5-49c6-abd0-f829df1c56d5",
      "metadata": {
        "id": "c384828f-0fe5-49c6-abd0-f829df1c56d5"
      },
      "outputs": [],
      "source": [
        "# load fits files\n",
        "data_dir = \"data_names/Sample_Data/\" ### Not full dataset\n",
        "# data_dir = \"data_names/Full_Train_Data/\" ### Full dataset\n",
        "data_names = os.listdir(data_dir)\n",
        "# make sure there aren't any weird files in the folder\n",
        "data_names = np.array([x for x in data_names if \".fits\" in x])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e724c7f2-5124-465b-957d-0e850c0f2b2c",
      "metadata": {
        "id": "e724c7f2-5124-465b-957d-0e850c0f2b2c"
      },
      "outputs": [],
      "source": [
        "# get run information\n",
        "run_nums = np.array([int(x.split(\"planet\")[1].split(\"_\")[0]) for x in data_names])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e7384638-8d6b-45a9-a952-bcfe002524ef",
      "metadata": {
        "id": "e7384638-8d6b-45a9-a952-bcfe002524ef"
      },
      "outputs": [],
      "source": [
        "# sort by run number\n",
        "order = np.argsort(run_nums)\n",
        "run_nums = run_nums[order]\n",
        "data_names = data_names[order]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download labels\n",
        "label_id = \"1gtBi4ILvCe8nTF09p_E9WWMplTQGC2Wr\"\n",
        "gdown.download(f\"https://drive.google.com/uc?id={label_id}\", \"label_name.csv\", quiet=False)"
      ],
      "metadata": {
        "id": "_ia8z2lQFPqG"
      },
      "id": "_ia8z2lQFPqG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cced4a9-2124-48f5-bb79-9d4cfe868f75",
      "metadata": {
        "id": "7cced4a9-2124-48f5-bb79-9d4cfe868f75"
      },
      "outputs": [],
      "source": [
        "# Load labels\n",
        "label_name = \"label_name.csv\"\n",
        "label_df = pd.read_csv(label_name, usecols=range(1, 11))\n",
        "label_df.head()\n",
        "# (run, number of planets, mass of planet 1, semimajor axis of planet 1, ....)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a6491a2-5f41-425f-a047-6e3455820517",
      "metadata": {
        "id": "1a6491a2-5f41-425f-a047-6e3455820517"
      },
      "outputs": [],
      "source": [
        "runs = label_df.run.to_numpy()\n",
        "Ns = label_df.n.to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1848803-56f7-4e79-b070-417dac9150c7",
      "metadata": {
        "id": "a1848803-56f7-4e79-b070-417dac9150c7"
      },
      "outputs": [],
      "source": [
        "# label whether it's a planet or not a planet\n",
        "labels = {}\n",
        "nums = {}\n",
        "for (name, run) in zip(data_names, run_nums):\n",
        "    label = Ns[np.where(runs == run)][0]\n",
        "    nums[name] = int(label)\n",
        "    labels[name] = int(label > 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abab1caa-99db-4fc6-9c00-6baf998fe8fb",
      "metadata": {
        "id": "abab1caa-99db-4fc6-9c00-6baf998fe8fb"
      },
      "outputs": [],
      "source": [
        "# Get actual data\n",
        "data = {}\n",
        "for name in data_names:\n",
        "    # There are 4 channels, but we only care about the first\n",
        "    data[name] = fits.open(f\"{data_dir}{name}\")[0].data.squeeze()[0]\n",
        "    # normalize\n",
        "    data[name] -= np.min(data[name])\n",
        "    data[name] /= np.max(data[name])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1d50c08e-81f3-4393-be87-1abb7b1dc108",
      "metadata": {
        "id": "1d50c08e-81f3-4393-be87-1abb7b1dc108"
      },
      "outputs": [],
      "source": [
        "# show some data\n",
        "# some of the data has zero values due to the orientation\n",
        "fig, axs = plt.subplots(ncols=2, nrows=2, figsize=((14, 14)))\n",
        "\n",
        "img_index = np.random.randint(0, len(data_names), size=4)\n",
        "\n",
        "axs[0, 0].imshow(data[data_names[img_index[0]]],\n",
        "          origin=\"lower\",\n",
        "          cmap=\"magma\",\n",
        "          norm=LogNorm(vmin=1e-6, vmax=1),\n",
        "          )\n",
        "axs[0, 1].imshow(data[data_names[img_index[1]]],\n",
        "          origin=\"lower\",\n",
        "          cmap=\"magma\",\n",
        "          norm=LogNorm(vmin=1e-6, vmax=1),\n",
        "          )\n",
        "axs[1, 0].imshow(data[data_names[img_index[2]]],\n",
        "          origin=\"lower\",\n",
        "          cmap=\"magma\",\n",
        "          norm=LogNorm(vmin=1e-6, vmax=1),\n",
        "          )\n",
        "axs[1, 1].imshow(data[data_names[img_index[3]]],\n",
        "          origin=\"lower\",\n",
        "          cmap=\"magma\",\n",
        "          norm=LogNorm(vmin=1e-6, vmax=1),\n",
        "          )\n",
        "\n",
        "axs[0, 0].set_title(f\"{data_names[img_index[0]]} ({nums[data_names[img_index[0]]]} planets)\")\n",
        "axs[0, 1].set_title(f\"{data_names[img_index[1]]} ({nums[data_names[img_index[1]]]} planets)\")\n",
        "axs[1, 0].set_title(f\"{data_names[img_index[2]]} ({nums[data_names[img_index[2]]]} planets)\")\n",
        "axs[1, 1].set_title(f\"{data_names[img_index[3]]} ({nums[data_names[img_index[3]]]} planets)\")\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c9bc492-220c-4146-8bd6-2a90b7008df0",
      "metadata": {
        "id": "0c9bc492-220c-4146-8bd6-2a90b7008df0"
      },
      "outputs": [],
      "source": [
        "xy_dim = data[data_names[0]].shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "362ff36e-334d-42bd-82fa-11f380e4b8e9",
      "metadata": {
        "id": "362ff36e-334d-42bd-82fa-11f380e4b8e9"
      },
      "outputs": [],
      "source": [
        "# Initialize the data arrays\n",
        "X = np.empty((len(data_names), xy_dim, xy_dim))\n",
        "y = np.empty((len(data_names), 1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c2b864e-b735-430e-8c13-3add1752b0a5",
      "metadata": {
        "id": "9c2b864e-b735-430e-8c13-3add1752b0a5"
      },
      "outputs": [],
      "source": [
        "# load images and labels\n",
        "for i, name in enumerate(data_names):\n",
        "    X[i, :, :] = data[name]\n",
        "    y[i, 0] = labels[name]\n",
        "# add a channel axis\n",
        "X = X[:, np.newaxis, :, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd00f485-21eb-45ad-9c00-f4dcabc4c6e9",
      "metadata": {
        "id": "fd00f485-21eb-45ad-9c00-f4dcabc4c6e9"
      },
      "outputs": [],
      "source": [
        "X = X.astype(np.float32)\n",
        "y = y.astype(np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16670a8b-680d-4a25-8637-0e6f36ec3f45",
      "metadata": {
        "id": "16670a8b-680d-4a25-8637-0e6f36ec3f45"
      },
      "outputs": [],
      "source": [
        "# split into train/test/val\n",
        "##### Report the AUC of the created test data\n",
        "test_split = 0.2\n",
        "val_split = 0.2\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_split)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=val_split)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "38de40e5-11a2-41dc-963c-300d9e72d7b0",
      "metadata": {
        "id": "38de40e5-11a2-41dc-963c-300d9e72d7b0"
      },
      "source": [
        "# Make datasets/loaders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3726bf7-6fc3-4eb4-b505-93547b293880",
      "metadata": {
        "id": "c3726bf7-6fc3-4eb4-b505-93547b293880"
      },
      "outputs": [],
      "source": [
        "class DiskDataset(Dataset):\n",
        "\n",
        "    \"\"\"Data loader\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        X: np.ndarray,\n",
        "        y: np.ndarray,\n",
        "        transform: list = None,\n",
        "        accelerator_name: str = \"mps\",\n",
        "    ) -> None:\n",
        "\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "        self.transform = transform\n",
        "\n",
        "        if accelerator_name == \"mps\":\n",
        "            self.device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "        elif accelerator_name == \"cuda:0\":\n",
        "            self.device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx) -> torch.Tensor:\n",
        "        x_, y_ = self.X[idx], self.y[idx]\n",
        "\n",
        "        x_, y_ = torch.from_numpy(x_),\\\n",
        "                torch.from_numpy(y_)\n",
        "\n",
        "        if self.transform:\n",
        "            x_ = self.transform(x_)\n",
        "        return x_.to(self.device), y_.to(self.device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "abd5347b-c575-4618-9e06-75009dcfec08",
      "metadata": {
        "id": "abd5347b-c575-4618-9e06-75009dcfec08"
      },
      "outputs": [],
      "source": [
        "# need to resize for EffnetV2\n",
        "input_size = 224\n",
        "transform = T.Compose([\n",
        "                        T.Resize((input_size, input_size), antialias=True),\n",
        "                        T.Normalize(mean=[0.5], std=[0.5]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c20e6b83-8961-4fa1-a30a-0a1ea5f569b0",
      "metadata": {
        "id": "c20e6b83-8961-4fa1-a30a-0a1ea5f569b0"
      },
      "outputs": [],
      "source": [
        "##### Now we actually make the dataset and dataloader in PyTorch fashion\n",
        "train_data = DiskDataset(X_train, y_train, transform=transform)\n",
        "val_data = DiskDataset(X_val, y_val, transform=transform)\n",
        "test_data = DiskDataset(X_test, y_test, transform=transform)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "77274b2b-3ded-4dfb-89d4-de58af35a427",
      "metadata": {
        "id": "77274b2b-3ded-4dfb-89d4-de58af35a427"
      },
      "outputs": [],
      "source": [
        "# this is artificially small due to the tiny amount of data in this example\n",
        "batch_size = 16\n",
        "\n",
        "# make the loader\n",
        "train_loader = DataLoader(train_data, shuffle=True, batch_size=batch_size)\n",
        "val_loader = DataLoader(val_data, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "af3c9b5a-da94-42f3-bc28-6a0feebc29bc",
      "metadata": {
        "id": "af3c9b5a-da94-42f3-bc28-6a0feebc29bc"
      },
      "source": [
        "# Make model\n",
        "\n",
        "This model is a simple implementation of torchvision's EfficientNetV2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de466952-a71f-4a14-a7fb-3dc3f2a6c905",
      "metadata": {
        "id": "de466952-a71f-4a14-a7fb-3dc3f2a6c905"
      },
      "outputs": [],
      "source": [
        "class CustomEfficientNetV2(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 num_channels: int = 1,\n",
        "                 num_outputs: int = 1,\n",
        "                 lr: float = 5e-4,\n",
        "                 xy_dim: int = 224,\n",
        "                ):\n",
        "        super().__init__()\n",
        "        self.save_hyperparameters()\n",
        "\n",
        "        # Load EfficientNetV2 model\n",
        "        self.model = torchvision.models.efficientnet_v2_s()\n",
        "\n",
        "        # Modify the first convolutional layer if input channels are different from 3\n",
        "        if num_channels != 3:\n",
        "            self.model.features[0][0] = nn.Conv2d(num_channels,\n",
        "                                                  self.model.features[0][0].out_channels,\n",
        "                                                  kernel_size=self.model.features[0][0].kernel_size,\n",
        "                                                  stride=self.model.features[0][0].stride,\n",
        "                                                  padding=self.model.features[0][0].padding,\n",
        "                                                  bias=False,\n",
        "                                                 )\n",
        "\n",
        "        # Modify the final fully connected layer\n",
        "        in_features = self.model.classifier[1].in_features\n",
        "        self.model.classifier[1] = nn.Linear(in_features, num_outputs)\n",
        "\n",
        "        self.criterion = nn.BCEWithLogitsLoss()\n",
        "\n",
        "        # Initialize containers to store outputs\n",
        "        self.validation_outputs = []\n",
        "        self.test_outputs = []\n",
        "\n",
        "        self.example_input_array = torch.randn((1, num_channels, xy_dim, xy_dim)).float()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "    def _process_batch(self, batch, when: str = \"train\"):\n",
        "        x, y = batch\n",
        "        y_hat = self(x)\n",
        "        loss = self.criterion(y_hat, y)\n",
        "        self.log(f\"{when}_loss\", loss)\n",
        "        if when != \"train\":\n",
        "            return {f\"{when}_loss\": loss, \"y_hat\": y_hat, \"y\": y}\n",
        "        return loss\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "        return self._process_batch(batch, when=\"train\")\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "        outputs = self._process_batch(batch, when=\"val\")\n",
        "        self.validation_outputs.append(outputs)\n",
        "        return outputs\n",
        "\n",
        "    def test_step(self, batch, batch_idx):\n",
        "        outputs = self._process_batch(batch, when=\"test\")\n",
        "        self.test_outputs.append(outputs)\n",
        "        return outputs\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "        return torch.optim.Adam(self.parameters(),\n",
        "                                lr=self.hparams.lr,\n",
        "                               )\n",
        "\n",
        "    def _roc_epoch_end(self, outputs, when: str = \"val\"):\n",
        "        \"\"\"Logs AUC during validation/testing\"\"\"\n",
        "        y_hat = torch.cat([x[\"y_hat\"] for x in outputs]).detach().cpu().numpy()\n",
        "        y = torch.cat([x[\"y\"] for x in outputs]).detach().cpu().numpy()\n",
        "        auc = self.calculate_auc(y_hat, y)\n",
        "        self.log(f\"{when}_auc\", auc)\n",
        "\n",
        "    def on_validation_epoch_end(self,):\n",
        "        self._roc_epoch_end(self.validation_outputs, when=\"val\")\n",
        "        self.validation_outputs.clear()\n",
        "\n",
        "    def on_test_epoch_end(self,):\n",
        "        self._roc_epoch_end(self.test_outputs, when=\"test\")\n",
        "        self.test_outputs.clear()\n",
        "\n",
        "    def calculate_auc(self, y_hat, y):\n",
        "        # Apply sigmoid to predictions if using BCEWithLogitsLoss\n",
        "        y_hat = torch.sigmoid(torch.tensor(y_hat).float()).numpy()\n",
        "        auc = roc_auc_score(y, y_hat)\n",
        "        return auc.astype(np.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0ea8b0be-7b82-4c6a-b748-c9c130337bb5",
      "metadata": {
        "id": "0ea8b0be-7b82-4c6a-b748-c9c130337bb5"
      },
      "outputs": [],
      "source": [
        "model = CustomEfficientNetV2()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "57a3ea10-2d58-447b-9f42-50396088e0a0",
      "metadata": {
        "jp-MarkdownHeadingCollapsed": true,
        "id": "57a3ea10-2d58-447b-9f42-50396088e0a0"
      },
      "source": [
        "# Train\n",
        "\n",
        "Full deployment will take a long time if GPUs aren't used."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "933232c7-4423-4e41-aeb5-3dff79d36905",
      "metadata": {
        "id": "933232c7-4423-4e41-aeb5-3dff79d36905"
      },
      "outputs": [],
      "source": [
        "accelerator_name = \"cuda:0\"\n",
        "\n",
        "if accelerator_name == \"mps\":\n",
        "    device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
        "elif accelerator_name == \"cuda:0\":\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "\n",
        "# Ensure that all operations are deterministic on GPU (if used) for reproducibility\n",
        "torch.backends.cudnn.determinstic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e8d6a6eb-b723-4090-ac68-04d5d8926d9a",
      "metadata": {
        "id": "e8d6a6eb-b723-4090-ac68-04d5d8926d9a"
      },
      "outputs": [],
      "source": [
        "#### necessary for newer PTL versions\n",
        "devices = 1\n",
        "accelerator = \"gpu\" if devices == 1 else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b38ff0e6-4e3c-4713-b5b4-182ab5f89031",
      "metadata": {
        "id": "b38ff0e6-4e3c-4713-b5b4-182ab5f89031"
      },
      "outputs": [],
      "source": [
        "#### This is artificially small for the purposes of speed\n",
        "num_epochs = 5\n",
        "# make the trainer\n",
        "trainer = pl.Trainer(\n",
        "    devices=devices,\n",
        "    accelerator=accelerator,\n",
        "    max_epochs=num_epochs,\n",
        "    log_every_n_steps=1,\n",
        "    callbacks=[\n",
        "        LearningRateMonitor(\"epoch\"),\n",
        "        progress.TQDMProgressBar(refresh_rate=1),\n",
        "        EarlyStopping(\n",
        "            monitor=\"val_auc\",\n",
        "            min_delta=0,\n",
        "            patience=20,\n",
        "            verbose=False,\n",
        "            mode=\"min\",\n",
        "        ),\n",
        "    ],\n",
        ")\n",
        "trainer.logger._log_graph = True\n",
        "trainer.logger._default_hp_metric = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f26c9bcd-42ab-4f69-b05d-e1a174b9e63e",
      "metadata": {
        "id": "f26c9bcd-42ab-4f69-b05d-e1a174b9e63e"
      },
      "outputs": [],
      "source": [
        "model = model.to(device)\n",
        "\n",
        "# fit the model\n",
        "trainer.fit(\n",
        "    model,\n",
        "    train_dataloaders=train_loader,\n",
        "    val_dataloaders=val_loader,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f51dae43-3082-4fa7-8c84-03b46635e9fe",
      "metadata": {
        "id": "f51dae43-3082-4fa7-8c84-03b46635e9fe"
      },
      "source": [
        "# Test\n",
        "\n",
        "This is the AUC that will be judged, i.e., provided data that isn't used in training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "435bb284-56f3-476e-8f7e-71dc7c163358",
      "metadata": {
        "id": "435bb284-56f3-476e-8f7e-71dc7c163358"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "print(\"Testing model\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Make a dataloader for the test data\n",
        "test_batch_size = 16\n",
        "test_loader = DataLoader(test_data, batch_size=test_batch_size)"
      ],
      "metadata": {
        "id": "3DexpO_qIja_"
      },
      "id": "3DexpO_qIja_",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# do inference on all test batches\n",
        "results = []\n",
        "\n",
        "for X_batch in test_loader:\n",
        "    X_batch = X_batch[0]  # DataLoader returns a tuple\n",
        "    with torch.no_grad():\n",
        "        outputs = torch.sigmoid(model(X_batch))\n",
        "    batch_results = outputs.detach().numpy().squeeze()\n",
        "    results.append(batch_results)\n",
        "\n",
        "y_pred = np.concatenate(results, axis=0)"
      ],
      "metadata": {
        "id": "HPb9HORPJXA0"
      },
      "id": "HPb9HORPJXA0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fe10ea9c-9f28-47d2-a3a4-3271080f9dbd",
      "metadata": {
        "id": "fe10ea9c-9f28-47d2-a3a4-3271080f9dbd"
      },
      "outputs": [],
      "source": [
        "# get ROC curve/AUC\n",
        "fpr, tpr, _ = roc_curve(y_test, y_pred)\n",
        "auc = roc_auc_score(y_test, y_pred)\n",
        "accuracy = np.sum([round(y_pred[i]) == y_test[i] for i in range(len(y_test))]) / len(y_test)\n",
        "\n",
        "print(f\"Accuracy of {accuracy:.2}. AUC of {auc}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "86df695d-45b5-4760-8027-1171f8b07463",
      "metadata": {
        "id": "86df695d-45b5-4760-8027-1171f8b07463"
      },
      "outputs": [],
      "source": [
        "# Plot ROC curve\n",
        "plt.figure(figsize=(10., 7.5))\n",
        "\n",
        "plt.plot(fpr, tpr, lw=3, c=\"steelblue\")\n",
        "plt.plot(np.linspace(0, 1, 100), np.linspace(0, 1, 100),\n",
        "         c=\"gray\", ls=\"--\", alpha=0.5, lw=3,\n",
        "         )\n",
        "\n",
        "plt.xlabel(\"FPR\", fontsize=14)\n",
        "plt.ylabel(\"TRP\", fontsize=14)\n",
        "\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f5fb3868-b6e3-45f1-8ea0-d44e29632e3c",
      "metadata": {
        "id": "f5fb3868-b6e3-45f1-8ea0-d44e29632e3c"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.10"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}